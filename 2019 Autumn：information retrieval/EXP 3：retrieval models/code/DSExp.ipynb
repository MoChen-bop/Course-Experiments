{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Stemmer\n",
    "import os\n",
    "import re\n",
    "from functools import partial\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \n",
    "    def __init__(self, enable_case_folding=True, enable_remove_stop_words=True,\n",
    "                enable_stemmer=False, enable_lemmatizer=True, min_length=2):\n",
    "        self.steps = []\n",
    "        self.SPLIT_WORDS_PATTERN = re.compile(r'\\s|\\.|\\:|\\?|\\(|\\)|\\[|\\]|\\{|\\}|\\<|\\>|\\'|\\!|\\\"|\\-|,|;|\\$|\\*|\\%|#')\n",
    "        self.steps.append(self.__split_words)\n",
    "        if enable_case_folding:\n",
    "            self.steps.append(self.__case_folding)\n",
    "        \n",
    "        if enable_remove_stop_words:\n",
    "            self.steps.append(self.__remove_stop_words)\n",
    "            self.stop_words = {'a', 'able', 'about', 'across', 'after', 'all',\n",
    "                               'almost', 'also', 'am', 'among', 'an', 'and',\n",
    "                               'any', 'are', 'as', 'at', 'be', 'because', 'been', \n",
    "                               'but', 'by', 'can', 'cannot', 'could', 'dear', 'did', \n",
    "                               'do', 'does', 'either', 'else', 'ever', 'every', 'for',\n",
    "                               'from', 'get', 'got', 'had', 'has', 'have', 'he', 'her',\n",
    "                               'hers', 'him', 'his', 'how', 'however', 'i', 'if', 'in', \n",
    "                               'into', 'is', 'it', 'its', 'just', 'least', 'let', 'like',\n",
    "                               'likely', 'may', 'me', 'might', 'most', 'must', 'my', 'neither',\n",
    "                               'no', 'nor', 'not', 'of', 'off', 'often', 'on', 'only', 'or',\n",
    "                               'other', 'our', 'own', 'rather', 'said', 'say', 'says', 'she', \n",
    "                               'should', 'since', 'so', 'some', 'than', 'that', 'the', 'their',\n",
    "                               'them', 'then', 'there', 'these', 'they', 'this', 'tis', 'to', \n",
    "                               'too', 'twas', 'us', 'wants', 'was', 'we', 'were', 'what', 'when',\n",
    "                               'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with',\n",
    "                               'would', 'yet', 'you', 'your'}\n",
    "        \n",
    "        if enable_stemmer:\n",
    "            self.steps.append(self.__stem)\n",
    "            self.stemmer = Stemmer.Stemmer('english')\n",
    "        \n",
    "        if enable_lemmatizer:\n",
    "            self.steps.append(self.__lemmatiza)\n",
    "            self.lemmatizer = WordNetLemmatizer()\n",
    "            \n",
    "        if min_length:\n",
    "            self.steps.append(lambda words: self.__remove_short_words(words, min_length))\n",
    "            \n",
    "            \n",
    "    def process(self, words):\n",
    "        for i, step in enumerate(self.steps):\n",
    "            words = list(step(words))\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    \n",
    "    def __split_words(self, words):\n",
    "        return list(filter(lambda word: word != '', self.SPLIT_WORDS_PATTERN.split(words)))\n",
    "    \n",
    "    def __case_folding(self, words):\n",
    "        return map(lambda word: word.casefold(), words)\n",
    "    \n",
    "    \n",
    "    def __remove_stop_words(self, words):\n",
    "        return filter(lambda word: word not in self.stop_words, words)\n",
    "    \n",
    "    \n",
    "    def __stem(self, words):\n",
    "        return map(lambda word: self.stemmer.stemWord(word), words)\n",
    "    \n",
    "    \n",
    "    def __lemmatiza(self, words):\n",
    "        return map(lambda word: self.lemmatizer.lemmatize(word), words)\n",
    "    \n",
    "    \n",
    "    def __remove_short_words(self, words, min_length):\n",
    "        return filter(lambda word: len(word) >= min_length, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Indexer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer:\n",
    "    \n",
    "    def __init__(self, preprocessor=None, field='content',\n",
    "                 output_file_path='.\\\\index'):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.output_file_path = output_file_path\n",
    "        self.dictionary = {}\n",
    "        self.field_name = field\n",
    "        self.document_num = 0\n",
    "        if field == 'content':\n",
    "            self.field = 3\n",
    "        elif field == 'title':\n",
    "            self.field = 2\n",
    "        elif field == 'authors':\n",
    "            self.field = 4\n",
    "        elif field == 'keywords':\n",
    "            self.field = 5\n",
    "        elif field == 'publication':\n",
    "            self.field = 7\n",
    "        \n",
    "        \n",
    "    def index(self, corpus):\n",
    "        self.document_num = len(corpus.documents)\n",
    "        for document in corpus.documents:\n",
    "            docID = document.docID\n",
    "            words_stream = document[self.field]\n",
    "            if isinstance(words_stream, list):\n",
    "                words = self.__generate_tokens(' '.join(words_stream))\n",
    "            else:\n",
    "                words = self.__generate_tokens(words_stream)\n",
    "            self.__add_into_dictionary(docID, words) \n",
    "        self.__flush_index_entry()\n",
    "            \n",
    "        \n",
    "    def get_posting_list(self, word):\n",
    "        if word in self.dictionary.keys():\n",
    "            return dictionary[word]\n",
    "        else:\n",
    "            return { }\n",
    "    \n",
    "    \n",
    "    def get_term_DF(self, word):\n",
    "        if word in self.dictionary.keys():\n",
    "            return len(self.dictionary[word])\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def get_doc_TF(self, docID, word):\n",
    "        posting_list = get_posting_list(word)\n",
    "        if docID in posting_list.keys():\n",
    "            return posting_list[docID]\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    \n",
    "    def generate_tokens(self, words_stream):\n",
    "        if self.preprocessor is not None:\n",
    "            words = self.preprocessor.process(words_stream)\n",
    "        return words\n",
    "    \n",
    "        \n",
    "    def __generate_tokens(self, words_stream):\n",
    "        if self.preprocessor is not None:\n",
    "            words = self.preprocessor.process(words_stream)\n",
    "        return words\n",
    "    \n",
    "    \n",
    "    def __add_into_dictionary(self, docID, words):\n",
    "        for word in words:\n",
    "            if word in self.dictionary.keys():\n",
    "                posting_list = self.dictionary[word]\n",
    "                if docID in posting_list.keys():\n",
    "                    posting_list[docID] += 1\n",
    "                else:\n",
    "                    posting_list[docID] = 1\n",
    "            else:\n",
    "                posting_list = {docID : 1}\n",
    "                self.dictionary[word] = posting_list            \n",
    "    \n",
    "        \n",
    "    def __flush_index_entry(self):\n",
    "        if not os.path.exists(self.output_file_path):\n",
    "            os.mkdir(self.output_file_path)\n",
    "        index_file_name = self.output_file_path + \"\\\\\" + self.field_name + \".index\"\n",
    "        index_file = open(index_file_name, 'w', encoding='utf-8')\n",
    "        for term, posting_list in self.dictionary.items():\n",
    "            self.__write_index_entry(index_file, term, posting_list)\n",
    "        index_file.close()\n",
    "        \n",
    "        \n",
    "    def __write_index_entry(self, file, term, posting_list):\n",
    "        posting = list(map(lambda e: '{}|{}'.format(e[0], e[1]),\n",
    "                           posting_list.items()))\n",
    "        line = '{}\\t{}\\t{}\\n'.format(term, str(len(posting_list)), ','.join(posting))\n",
    "        file.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    \n",
    "    def __init__(self, extractor):\n",
    "        self.extractor = extractor\n",
    "        self.documents = []\n",
    "        \n",
    "        \n",
    "    def build(self, documents_path):\n",
    "        documents = []\n",
    "        if os.path.isdir(documents_path):\n",
    "            for root, _, file in os.walk(documents_path):\n",
    "                document_file = os.path.join(root, file)\n",
    "                documents += extractor.extract(document_file)\n",
    "        else:\n",
    "            documents += extractor.extract(documents_path)\n",
    "        \n",
    "        self.documents = documents\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OHSUMED_Extractor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, namedtuple, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OHSUMED_Extractor():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def extract(self, documents_file):\n",
    "        file = open(documents_file, 'r', encoding='utf-8')\n",
    "        Document = namedtuple('Document', ['docID', 'sID', 'title',  'content', \n",
    "                                           'authors', 'keyswords', 'pType', 'publication'])\n",
    "        documents = []\n",
    "        not_finish = True\n",
    "        \n",
    "        docID = 0\n",
    "        sID = 0\n",
    "        title = ''\n",
    "        content = ''\n",
    "        authors = []\n",
    "        keywords = []\n",
    "        pType = ''\n",
    "        publication = ''\n",
    "        source = ''\n",
    "        \n",
    "        have_content = False\n",
    "        while not_finish:\n",
    "            line = file.readline()\n",
    "            if line == None or len(line) < 2:\n",
    "                not_finish = False\n",
    "                break\n",
    "            tag = line[1]\n",
    "            if tag == 'I':\n",
    "                sID = int(line[3:])\n",
    "            elif tag == 'U':\n",
    "                line = file.readline()\n",
    "                docID = int(line.strip())\n",
    "            elif tag == 'S':\n",
    "                line = file.readline()\n",
    "                publication = [line.strip()]\n",
    "            elif tag == 'M':\n",
    "                line = file.readline()\n",
    "                line = line.strip().strip('.')\n",
    "                keywords = [word.split('/')[0].strip() for word in line.split(';')]\n",
    "            elif tag == 'T':\n",
    "                line = file.readline()\n",
    "                title = line.strip()\n",
    "            elif tag == 'P':\n",
    "                line = file.readline()\n",
    "                pType = line.strip()\n",
    "            elif tag == 'W':\n",
    "                have_content = True\n",
    "                line = file.readline()\n",
    "                content = line.strip()\n",
    "            elif tag == 'A':\n",
    "                line = file.readline().strip().strip('.')\n",
    "                authors = [ author.strip() for author in line.split(';')]\n",
    "                if have_content:\n",
    "                    documents.append(Document(docID, sID, title, content, \n",
    "                                              authors, keywords, pType, publication))\n",
    "                have_content = False\n",
    "                    \n",
    "        file.close()\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boolean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BooleanModel:\n",
    "    \n",
    "    def __init__(self, indexer):\n",
    "        self.indexer = indexer\n",
    "    \n",
    "    \n",
    "    def search(self, query, topK=20, normalized=False):\n",
    "        query_posting_list = self.__vectorize(query)\n",
    "        if normalized:\n",
    "            document_scores = self.__documents_normalized_score(query_posting_list)\n",
    "        else:\n",
    "            document_scores = self.__documents_score(query_posting_list)\n",
    "\n",
    "        return document_scores[ : topK]\n",
    "        \n",
    "        \n",
    "    def __vectorize(self, content):\n",
    "        words = self.indexer.generate_tokens(content)\n",
    "        posting_list = {}\n",
    "        for word in words:\n",
    "            posting_list[word] = 1\n",
    "        return posting_list\n",
    "    \n",
    "    \n",
    "    def __score_tf(self, posting_list1, posting_list2):        \n",
    "        score = 0\n",
    "        for term, tf1 in posting_list1.items():\n",
    "            if term in posting_list2.keys():\n",
    "                score += 1\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    def __score_normalized_tf(self, posting_list1, posting_list2):\n",
    "        score = 0\n",
    "        norm1 = len(posting_list1)\n",
    "        norm2 = len(posting_list2)\n",
    "        \n",
    "        for term, tf1 in posting_list1.items():\n",
    "            if term in posting_list2.keys():\n",
    "                score += 1\n",
    "        score = score / (math.sqrt(norm1) * math.sqrt(norm2))\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    def __documents_score(self, query_posting_list):\n",
    "        document_scores = Counter()\n",
    "        for term, query_tf in query_posting_list.items():\n",
    "            documents = self.indexer.dictionary.get(term, { })\n",
    "            for docID, _ in documents.items():\n",
    "                document_scores[docID] += 1\n",
    "        document_scores = list(document_scores.items())\n",
    "        document_scores.sort(key=lambda ds: ds[1], reverse=True)\n",
    "        return document_scores\n",
    "\n",
    "    \n",
    "    def __documents_normalized_score(self, query_posting_list):\n",
    "        document_scores = Counter()\n",
    "        document_norms = Counter()\n",
    "        query_norm = len(query_posting_list)\n",
    "        \n",
    "        for _, documents in self.indexer.dictionary.items():\n",
    "            for docID, _ in documents.items():\n",
    "                document_norms[docID] += 1\n",
    "        \n",
    "        for term, query_tf in query_posting_list.items():\n",
    "            if term in self.indexer.dictionary.keys():\n",
    "                documents = self.indexer.dictionary[term]\n",
    "                for docID, tf in documents.items():\n",
    "                    document_scores[docID] += 1\n",
    "\n",
    "        query_norm = math.sqrt(query_norm)\n",
    "        for docID in document_scores:\n",
    "            document_norm = math.sqrt(document_norms[docID])\n",
    "            document_scores[docID] /= (document_norm * query_norm)\n",
    "            \n",
    "        document_scores = list(document_scores.items())\n",
    "        document_scores.sort(key=lambda ds: ds[1], reverse=True)\n",
    "        return document_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VSM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFModel:\n",
    "    \n",
    "    def __init__(self, corpus, indexer):\n",
    "        self.indexer = indexer\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    \n",
    "    def search(self, query, topK=20, normalized=False):\n",
    "        query_posting_list = self.__vectorize(query)\n",
    "        if normalized:\n",
    "            document_scores = self.__documents_normalized_score(query_posting_list)\n",
    "        else:\n",
    "            document_scores = self.__documents_score(query_posting_list)\n",
    "\n",
    "        return document_scores[ : topK]\n",
    "        \n",
    "\n",
    "    def similarity(self, content1, content2, normalized=False):\n",
    "        posting_list1 = self.__vectorize(content1)\n",
    "        posting_list2 = self.__vectorize(content2)\n",
    "        \n",
    "        if len(posting_list2) < len(posting_list1):\n",
    "            temp = posting_list1\n",
    "            posting_list1 = posting_list2\n",
    "            posting_list2 = temp\n",
    "        if normalized:\n",
    "            return self.__score_normalized_tf(posting_list1, posting_list2)\n",
    "        else:\n",
    "            return self.__score_tf(posting_list1, posting_list2)\n",
    "        \n",
    "        \n",
    "    def __vectorize(self, content):\n",
    "        words = self.indexer.generate_tokens(content)\n",
    "        posting_list = {}\n",
    "        for word in words:\n",
    "            if word in posting_list.keys():\n",
    "                posting_list[word] += 1\n",
    "            else:\n",
    "                posting_list[word] = 1\n",
    "        return posting_list\n",
    "\n",
    "    \n",
    "    def __score_tf(self, posting_list1, posting_list2):        \n",
    "        score = 0\n",
    "        for term, tf1 in posting_list1.items():\n",
    "            if term in posting_list2.keys():\n",
    "                score += posting_list2[term] * tf1\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    def __score_normalized_tf(self, posting_list1, posting_list2):\n",
    "        score = 0\n",
    "        norm1 = 0\n",
    "        norm2 = 0\n",
    "        for _, tf in posting_list2.items():\n",
    "            norm2 += tf * tf\n",
    "        \n",
    "        for term, tf1 in posting_list1.items():\n",
    "            norm1 += tf1 * tf1\n",
    "            if term in posting_list2.keys():\n",
    "                score += posting_list2[term] * tf1\n",
    "        score = score / (math.sqrt(norm1) * math.sqrt(norm2))\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    def __documents_score(self, query_posting_list):\n",
    "        document_scores = Counter()\n",
    "        for term, query_tf in query_posting_list.items():\n",
    "            documents = self.indexer.dictionary.get(term, {})\n",
    "            for docID, tf in documents.items():\n",
    "                document_scores[docID] += tf * query_tf\n",
    "        document_scores = list(document_scores.items())\n",
    "        document_scores.sort(key=lambda ds: ds[1], reverse=True)\n",
    "        return document_scores\n",
    "    \n",
    "    \n",
    "    def __documents_normalized_score(self, query_posting_list):\n",
    "        document_scores = Counter()\n",
    "        document_norms = Counter()\n",
    "        query_norm = 0\n",
    "        \n",
    "        for term, query_tf in query_posting_list.items():\n",
    "            query_norm += query_tf * query_tf\n",
    "            documents = self.indexer.dictionary[term]\n",
    "            for docID, tf in documents.items():\n",
    "                document_norms[docID] += tf * tf\n",
    "                document_scores[docID] += tf * query_tf\n",
    "        \n",
    "        query_norm = math.sqrt(query_norm)\n",
    "        for docID in document_scores:\n",
    "            document_norm = math.sqrt(document_norms[docID])\n",
    "            document_scores[docID] /= (document_norm * query_norm)\n",
    "            \n",
    "        document_scores = list(document_scores.items())\n",
    "        document_scores.sort(key=lambda ds: ds[1], reverse=True)\n",
    "        return document_scores\n",
    "    \n",
    "    \n",
    "    def __documents_normalized_score_2(self, query_posting_list):\n",
    "        document_scores = Counter()\n",
    "        document_norms = Counter()\n",
    "        query_norm = 0\n",
    "        \n",
    "        for _, documents in self.indexer.dictionary.items():\n",
    "            for docID, tf in documents.items():\n",
    "                document_norms[docID] += tf * tf\n",
    "        \n",
    "        for term, query_tf in query_posting_list.items():\n",
    "            query_norm += query_tf * query_tf\n",
    "            documents = self.indexer.dictionary[term]\n",
    "            for docID, tf in documents.items():\n",
    "                document_scores[docID] += tf * query_tf\n",
    "        \n",
    "        query_norm = math.sqrt(query_norm)\n",
    "        for docID in document_scores:\n",
    "            document_norm = math.sqrt(document_norms[docID])\n",
    "            document_scores[docID] /= (document_norm * query_norm)\n",
    "            \n",
    "        document_scores = list(document_scores.items())\n",
    "        document_scores.sort(key=lambda ds: ds[1], reverse=True)\n",
    "        return document_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF_IDFModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_IDFModel:\n",
    "    \n",
    "    def __init__(self, corpus, indexer, args):\n",
    "        self.corpus = corpus\n",
    "        self.indexer = indexer\n",
    "        if args[0] == 'add':\n",
    "            self.__TF_IDF = partial(self.__TF_IDF_add, alpha=args[1])\n",
    "        elif args[0] == 'mul':\n",
    "            self.__TF_IDF = partial(self.__TF_IDF_mul, a=args[1], b=args[2])\n",
    "        elif args == 'log':\n",
    "            self.__TF_IDF = self.__TF_IDF_log\n",
    "    \n",
    "    \n",
    "    def search(self, query, topK=20, normalized=False):\n",
    "        query_posting_list = self.__vectorize(query)\n",
    "        if normalized:\n",
    "            document_scores = self.__documents_normalized_score(query_posting_list)\n",
    "        else:\n",
    "            document_scores = self.__documents_score(query_posting_list)\n",
    "\n",
    "        return document_scores[ : topK]\n",
    "\n",
    "\n",
    "    def similarity(self, content1, content2, normalized=False):\n",
    "        posting_list1 = self.__vectorize(content1)\n",
    "        posting_list2 = self.__vectorize(content2)\n",
    "        \n",
    "        if len(posting_list2) < len(posting_list1):\n",
    "            temp = posting_list1\n",
    "            posting_list1 = posting_list2\n",
    "            posting_list2 = temp\n",
    "        if normalized:\n",
    "            return self.__score_normalized_tf_idf(posting_list1, posting_list2)\n",
    "        else:\n",
    "            return self.__score_tf_idf(posting_list1, posting_list2)\n",
    "        \n",
    "        \n",
    "    def __TF_IDF_add(self, tf, df, alpha):\n",
    "        idf = self.indexer.document_num / df\n",
    "        return idf * alpha + (1 - alpha) * tf\n",
    "    \n",
    "    \n",
    "    def __TF_IDF_mul(self, tf, df, a, b):\n",
    "        idf = self.indexer.document_num / df\n",
    "        return math.pow(idf, a) * math.pow(tf, b)\n",
    "        \n",
    "        \n",
    "    def __TF_IDF_log(self, tf, df):\n",
    "        idf = math.log(self.indexer.document_num / df)\n",
    "        return math.log(1 + tf) * idf       \n",
    "        \n",
    "    \n",
    "    def __vectorize(self, content):\n",
    "        words = self.indexer.generate_tokens(content)\n",
    "        posting_list = {}\n",
    "        for word in words:\n",
    "            if word in posting_list.keys():\n",
    "                posting_list[word] += 1\n",
    "            else:\n",
    "                posting_list[word] = 1\n",
    "        return posting_list\n",
    "\n",
    "    \n",
    "    def __score_tf_idf(self, posting_list1, posting_list2):        \n",
    "        score = 0\n",
    "        for term, tf1 in posting_list1.items():\n",
    "            df = self.indexer.get_term_DF(term)\n",
    "            tf_idf1 = self.__TF_IDF(tf1, df)\n",
    "            if term in posting_list2.keys():\n",
    "                tf_idf2 = self.__TF_IDF(posting_list2[term], df)\n",
    "                score +=  tf_idf1 * tf_idf2\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    def __score_normalized_tf_idf(self, posting_list1, posting_list2):\n",
    "        score = 0\n",
    "        norm1 = 0\n",
    "        norm2 = 0\n",
    "        for term, tf in posting_list2.items():\n",
    "            df = self.indexer.get_term_DF(term)\n",
    "            tf_idf = self.__TF_IDF(tf, df)\n",
    "            norm2 += tf_idf * tf_idf\n",
    "        \n",
    "        for term, tf1 in posting_list1.items():\n",
    "            df = self.indexer.get_term_DF(term)\n",
    "            tf_idf1 = self.__tf_idf(tf1, idf)\n",
    "            norm1 += tf_idf1 * tf_idf1\n",
    "            if term in posting_list2.keys():\n",
    "                tf_idf2 = self.__TF_IDF(posting_list2[term], df)\n",
    "                score += tf_idf1 * tf_idf2\n",
    "\n",
    "        score = score / (math.sqrt(norm1) * math.sqrt(norm2))\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    def __documents_score(self, query_posting_list):\n",
    "        document_scores = Counter()\n",
    "        for term, query_tf in query_posting_list.items():\n",
    "            df = self.indexer.get_term_DF(term)\n",
    "            query_tf_idf = self.__TF_IDF(query_tf, df)\n",
    "            documents = self.indexer.dictionary.get(term, {})\n",
    "            for docID, tf in documents.items():\n",
    "                tf_idf = self.__TF_IDF(tf, df)\n",
    "                document_scores[docID] += query_tf_idf * tf_idf\n",
    "        document_scores = list(document_scores.items())\n",
    "        document_scores.sort(key=lambda ds: ds[1], reverse=True)\n",
    "        return document_scores\n",
    "    \n",
    "    \n",
    "    def __documents_normalized_score(self, query_posting_list):\n",
    "        document_scores = Counter()\n",
    "        document_norms = Counter()\n",
    "        query_norm = 0\n",
    "        \n",
    "        for term, query_tf in query_posting_list.items():\n",
    "            df = self.indexer.get_term_DF(term)\n",
    "            query_tf_idf = self.__TF_IDF(query_tf, df)\n",
    "            query_norm += query_tf_idf * query_tf_idf\n",
    "            documents = self.indexer.dictionary[term]\n",
    "            for docID, tf in documents.items():\n",
    "                tf_idf = self.__TF_IDF(tf, df)\n",
    "                document_norms[docID] += tf_idf * tf_idf\n",
    "                document_scores[docID] += query_tf_idf * tf_idf\n",
    "        \n",
    "        query_norm = math.sqrt(query_norm)\n",
    "        for docID in document_scores:\n",
    "            document_norm = math.sqrt(document_norms[docID])\n",
    "            document_scores[docID] /= (document_norm * query_norm)\n",
    "            \n",
    "        document_scores = list(document_scores.items())\n",
    "        document_scores.sort(key=lambda ds: ds[1], reverse=True)\n",
    "        return document_scores\n",
    "    \n",
    "    \n",
    "    def __documents_normalized_score_2(self, query_posting_list):\n",
    "        document_scores = Counter()\n",
    "        document_norms = Counter()\n",
    "        query_norm = 0\n",
    "        \n",
    "        for term, documents in self.indexer.dictionary.items():\n",
    "            df = self.indexer.get_term_DF(term)\n",
    "            for docID, tf in documents.items():\n",
    "                tf_idf = self.__TF_IDF(tf, df)\n",
    "                document_norms[docID] += tf_idf * tf_idf\n",
    "        \n",
    "        for term, query_tf in query_posting_list.items():\n",
    "            df = self.indexer.get_term_DF(term)\n",
    "            query_tf_idf = self.__TF_IDF(query_tf, df)\n",
    "            query_norm += query_tf_idf * query_tf_idf\n",
    "            documents = self.indexer.dictionary[term]\n",
    "            for docID, tf in documents.items():\n",
    "                tf_idf = self.__TF_IDF(tf, df)\n",
    "                document_scores[docID] += query_tf_idf * tf_idf\n",
    "        \n",
    "        query_norm = math.sqrt(query_norm)\n",
    "        for docID in document_scores:\n",
    "            document_norm = math.sqrt(document_norms[docID])\n",
    "            document_scores[docID] /= (document_norm * query_norm)\n",
    "            \n",
    "        document_scores = list(document_scores.items())\n",
    "        document_scores.sort(key=lambda ds: ds[1], reverse=True)\n",
    "        return document_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSIModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pickle\n",
    "import numpy\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSIModel:\n",
    "    \n",
    "    def __init__(self, corpus, preprocessor, output_filename='.\\\\lsi', num_topics=50):\n",
    "        self.num_topics = num_topics\n",
    "        self.origin_corpus = corpus\n",
    "        self.docs = []\n",
    "        self.preprocessor = preprocessor\n",
    "        dict_suffix = 'ohsu'\n",
    "        corpus_suffix = 'ohsu'\n",
    "        self.output_filename = output_filename\n",
    "        self.dict_filename = output_filename + '\\\\%s.dict' % dict_suffix\n",
    "        self.corpus_filename = output_filename + '/%s.mm' % corpus_suffix\n",
    "        self.lsi_filename = output_filename + '\\\\%s_%s.lsi' % (corpus_suffix, num_topics)\n",
    "        self.index_filename = output_filename + '\\\\%s_%s.lsi.index' % (corpus_suffix, num_topics)\n",
    "        self.doc2id_filename = output_filename + \"\\\\%s.doc2id.pickle\" % corpus_suffix\n",
    "        self.id2doc_filename = output_filename + \"\\\\%s.id2doc.pickle\" % corpus_suffix\n",
    "        self._create_directories()\n",
    "        \n",
    "        \n",
    "    def _create_directories(self):\n",
    "        if not os.path.exists(self.output_filename):\n",
    "            os.mkdir(self.output_filename)\n",
    "            \n",
    "            \n",
    "    def _create_docs_dict(self, docs):\n",
    "        self.doc2id = dict(zip(docs, range(len(docs))))\n",
    "        self.id2doc = dict(zip(range(len(docs)), docs))\n",
    "        pickle.dump(self.doc2id, open(self.doc2id_filename, \"wb\"))\n",
    "        pickle.dump(self.id2doc, open(self.id2doc_filename, \"wb\"))\n",
    "        \n",
    "    \n",
    "    def _load_docs_dict(self):\n",
    "        self.doc2id = pickle.load(open(self.doc2id_filename, 'rb'))\n",
    "        self.id2doc = pickle.load(open(self.id2doc_filename, 'rb'))\n",
    "        \n",
    "        \n",
    "    def _generate_dictionary(self):\n",
    "        print(\"generating dictionary...\")\n",
    "        documents = []\n",
    "        for document in self.origin_corpus.documents:\n",
    "            tokens = preprocessor.process(document.content)\n",
    "            documents.append(tokens)\n",
    "        self.dictionary = corpora.Dictionary(documents)\n",
    "        self.dictionary.save(self.dict_filename)\n",
    "        \n",
    "        \n",
    "    def _load_dictionary(self, regenerate=False):\n",
    "        if not os.path.exists(self.dict_filename) or regenerate is True:\n",
    "            self._generate_dictionary()\n",
    "        else:\n",
    "            self.dictionary = corpora.Dictionary.load(self.dict_filename)\n",
    "        \n",
    "        \n",
    "    def _generate_corpus(self):\n",
    "        print(\"generating corpus...\")\n",
    "        self.corpus = []\n",
    "        corpus_memory_friendly = self._vectorize_corpus(self.origin_corpus, self.dictionary)\n",
    "        count = 0\n",
    "        for vector in corpus_memory_friendly:\n",
    "            self.corpus.append(vector)\n",
    "            count += 1\n",
    "            if count % 10000 == 0:\n",
    "                print(\"%d vectors processed\" % count)\n",
    "        self._create_docs_dict(self.docs)\n",
    "        corpora.MmCorpus.serialize(self.corpus_filename, self.corpus)\n",
    "        \n",
    "    \n",
    "    def _vectorize_corpus(self, corpus, dictionary):\n",
    "        for document in corpus.documents:\n",
    "            docID = document.docID\n",
    "            tokens = preprocessor.process(document.content)\n",
    "            self.docs.append(docID)\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "        \n",
    "    \n",
    "    def _vectorize(self, content):\n",
    "        tokens = self.preprocessor.process(content)\n",
    "        bow = self.dictionary.doc2bow(tokens)\n",
    "        return self.lsi[bow]\n",
    "        \n",
    "        \n",
    "    def _load_corpus(self, regenerate=False):\n",
    "        if not os.path.exists(self.corpus_filename) or regenerate is True:\n",
    "            self._generate_corpus()\n",
    "        else:\n",
    "            self.corpus = corpora.MmCorpus(self.corpus_filename)\n",
    "            \n",
    "            \n",
    "    def _generate_lsi_model(self, regenerate=False):\n",
    "        print(\"generating lsi models...\")\n",
    "        if not os.path.exists(self.lsi_filename) or regenerate is True:\n",
    "            self.lsi = models.LsiModel(self.corpus, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "            self.lsi.save(self.lsi_filename)\n",
    "            self.index = similarities.MatrixSimilarity(self.lsi[self.corpus])\n",
    "            self.index.save(self.index_filename)\n",
    "        elif not os.path.exists(self.index_filename):\n",
    "            self.lsi = models.LsiModel.load(self.lsi_filename)\n",
    "            self.index = similarities.MatrixSimilarity(self.lsi[self.corpus])\n",
    "            self.index.save(self.index_filename)\n",
    "            \n",
    "            \n",
    "    def _load_lsi_model(self, regenerate=False):\n",
    "        if os.path.exists(self.lsi_filename) and os.path.exists(self.index_filename) and regenerate is False:\n",
    "            self.lsi = models.LsiModel.load(self.lsi_filename)\n",
    "            self.index = similarities.MatrixSimilarity.load(self.index_filename)\n",
    "        else:\n",
    "            self._generate_lsi_model(regenerate)\n",
    "    \n",
    "    def load(self, regenerate=False):\n",
    "        self._load_dictionary(regenerate)\n",
    "        self._load_corpus(regenerate)\n",
    "        self._load_lsi_model(regenerate)\n",
    "        self._load_docs_dict()\n",
    "    \n",
    "    def _get_vector(self, doc):\n",
    "        vec_bow = None\n",
    "        try:\n",
    "            vec_bow = self.corpus[self.doc2id[doc]]\n",
    "        except KeyError:\n",
    "            print(\"Document '%s' does not exist. Have you used the proper string cleaner?\" % doc)\n",
    "        return vec_bow\n",
    "    \n",
    "    \n",
    "    def search(self, query, topK=20, normalized=False):\n",
    "        query_vector = self._vectorize(query)\n",
    "        query_sims = self.index[query_vector]\n",
    "        query_sims = sorted(enumerate(query_sims), key=lambda item: -item[1])[:topK]\n",
    "        sims = [(self.id2doc[docid], weight) for docid, weight in query_sims]\n",
    "        return sims\n",
    "    \n",
    "    \n",
    "    def get_similars(self, doc, num_sim=20):\n",
    "        vec_bow = self._get_vector(doc)\n",
    "        if vec_bow is None:\n",
    "            return []\n",
    "        vec_lsi = self.lsi[vec_bow]\n",
    "        sims = self.index[vec_lsi]\n",
    "        sims = sorted(enumerate(sims), key=lambda item: -item[1])[1:num_sim+1]\n",
    "        sims = [(self.id2doc[docid], weight) for docid, weight in sims]\n",
    "        return sims\n",
    "    \n",
    "    def get_pairwise_similarity(self, doc1, doc2):\n",
    "        vec_bow1 = self._get_vector(doc1)\n",
    "        vec_bow2 = self._get_vector(doc2)\n",
    "        if vec_bow1 is None or vec_bow2 is None:\n",
    "            return None\n",
    "        vec_lsi1 = [val for idx,val in self.lsi[vec_bow1]]\n",
    "        vec_lsi2 = [val for idx,val in self.lsi[vec_bow2]]\n",
    "        return cosine(vec_lsi1, vec_lsi2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TS_SSModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS_SS:\n",
    "    \n",
    "    def Cosine(self, vec1: np.ndarray, vec2: np.ndarray):\n",
    "        return np.dot(vec1, vec2.T) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    \n",
    "    \n",
    "    def VectorSize(self, vec: np.ndarray):\n",
    "        return np.linalg.norm(vec)\n",
    "    \n",
    "    \n",
    "    def Euclidean(self, vec1: np.ndarray, vec2: np.ndarray):\n",
    "        return np.linalg.norm(vec1 - vec2)\n",
    "    \n",
    "    \n",
    "    def Theta(self, vec1: np.ndarray, vec2: np.ndarray):\n",
    "        return np.arccos(self.Cosine(vec1, vec2)) + np.radians(10)\n",
    "    \n",
    "    \n",
    "    def Triangle(self, vec1: np.ndarray, vec2: np.ndarray):\n",
    "        theta = np.radians(self.Theta(vec1, vec2))\n",
    "        return (self.VectorSize(vec1) * self.VectorSize(vec2) * np.sin(theta)) / 2\n",
    "    \n",
    "    \n",
    "    def Magnitude_Difference(self, vec1: np.ndarray, vec2: np.ndarray):\n",
    "        return abs(self.VectorSize(vec1) - self.VectorSize(vec2))\n",
    "    \n",
    "    \n",
    "    def Sector(self, vec1: np.ndarray, vec2: np.ndarray):\n",
    "        ED = self.Euclidean(vec1, vec2)\n",
    "        MD = self.Magnitude_Difference(vec1, vec2)\n",
    "        theta = self.Theta(vec1, vec2)\n",
    "        return math.pi * (ED + MD)**2 * theta/360\n",
    "    \n",
    "    \n",
    "    def __call__(self, vec1: np.ndarray, vec2: np.ndarray):\n",
    "        return self.Triangle(vec1, vec2) * self.Sector(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS_SSModel:\n",
    "    \n",
    "    def __init__(self, corpus, indexer, preprocessor, measure='Cosine'):\n",
    "        self.origin_corpus = corpus\n",
    "        self.indexer = indexer\n",
    "        self.preprocessor = preprocessor\n",
    "        model = TS_SS()\n",
    "        self.measure_type = measure\n",
    "        if measure == 'Cosine':\n",
    "            self.SM = model.Cosine\n",
    "        elif measure == 'ED':\n",
    "            self.SM = model.Euclidean\n",
    "        elif measure == 'TS':\n",
    "            self.SM = model.Triangle\n",
    "        elif measure == 'SS':\n",
    "            self.SM = model.Sector\n",
    "        elif measure == 'TS-SS':\n",
    "            self.SM = model\n",
    "        print(\"building corpus...\")\n",
    "        self.__build_corpus()\n",
    "            \n",
    "    \n",
    "    def search(self, query, topK=20, normalized=False):\n",
    "        query_bow = self.__get_bow(query)\n",
    "        document_scores = self.__document_scores(query_bow)\n",
    "        return document_scores[ : topK]\n",
    "        \n",
    "        \n",
    "    def __build_corpus(self):\n",
    "        self.doc_bows = {}\n",
    "        for term, posting_list in self.indexer.dictionary.items():\n",
    "            df = len(posting_list)\n",
    "            for docID, tf in posting_list.items():\n",
    "                if docID not in self.doc_bows.keys():\n",
    "                    self.doc_bows[docID] = {}\n",
    "                self.doc_bows[docID][term] = self.__TF_IDF(tf, df)\n",
    "    \n",
    "    \n",
    "    def __get_bow(self, text):\n",
    "        tokens = self.preprocessor.process(text)\n",
    "        bow = {}\n",
    "        for token in tokens:\n",
    "            if token in bow:\n",
    "                bow[token] += 1\n",
    "            else:\n",
    "                bow[token] = 1\n",
    "        \n",
    "        for term in bow.keys():\n",
    "            df = len(self.indexer.dictionary[term])\n",
    "            bow[term] = self.__TF_IDF(bow[term], df)\n",
    "        return bow\n",
    "    \n",
    "        \n",
    "    def __vectorize(self, query_bow, doc_bow):\n",
    "        terms = set.union(set(query_bow.keys()), doc_bow.keys())\n",
    "        query_vector = []\n",
    "        doc_vector = []\n",
    "        for term in terms:\n",
    "            if term in query_bow.keys():\n",
    "                query_vector.append(query_bow[term])\n",
    "            else:\n",
    "                query_vector.append(0)\n",
    "            if term in doc_bow.keys():\n",
    "                doc_vector.append(doc_bow[term])\n",
    "            else:\n",
    "                doc_vector.append(0)\n",
    "        query_vector = np.array(query_vector)\n",
    "        doc_vector = np.array(doc_vector)\n",
    "        return query_vector, doc_vector\n",
    "        \n",
    "        \n",
    "    def __TF_IDF(self, tf, df):\n",
    "        idf = math.log(self.indexer.document_num / df)\n",
    "        return math.log(1 + tf) * idf       \n",
    "        \n",
    "    \n",
    "    def __document_scores(self, query_bow):\n",
    "        document_scores = Counter()\n",
    "        \n",
    "        for docID, doc_bow in self.doc_bows.items():\n",
    "            query_vector, doc_vector = self.__vectorize(query_bow, doc_bow)\n",
    "            score = self.SM(query_vector, doc_vector)\n",
    "            document_scores[docID] = score\n",
    "        \n",
    "        document_scores = list(document_scores.items())\n",
    "        if self.measure_type == 'Cosine':\n",
    "            document_scores.sort(key=lambda ds: ds[1], reverse=True)\n",
    "        else:\n",
    "            document_scores.sort(key=lambda ds: ds[1], reverse=False)\n",
    "        return document_scores\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WMDModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from pyemd import emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WMDModel:\n",
    "    \n",
    "    def __init__(self, corpus, indexer, preprocessor):\n",
    "        self.corpus = corpus\n",
    "        self.indexer = indexer\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model_path = \"C:\\\\Users\\\\dell\\\\Desktop\\\\DocumentSimilarity\\\\Word_Mover_Distance-master\\\\data\\\\word_model.mod\"\n",
    "        self.model = Word2Vec.load(self.model_path)\n",
    "\n",
    "    \n",
    "    def build(self):\n",
    "        self.__build_vocabulary()\n",
    "        self.__vectorize_corpus()\n",
    "        self.__calculate_word_distance()\n",
    "        \n",
    "    \n",
    "    def search(self, query, topK=20, normalized=False):\n",
    "        query_vector = self._vectorize(query)\n",
    "        document_scores = self.__documents_score(query_vector)\n",
    "        return document_scores[ : topK]\n",
    "        \n",
    "        \n",
    "    def _vectorize(self, content):\n",
    "        v = self.vectorizer.transform([content])[0]\n",
    "        v = v.toarray().ravel()\n",
    "        v = v.astype(np.double)\n",
    "        v /= v.sum()\n",
    "        return v\n",
    "        \n",
    "        \n",
    "    def __build_vocabulary(self):\n",
    "        print(\"building vocabulary...\")\n",
    "        self.vocabulary = []\n",
    "        for word, _ in self.indexer.dictionary.items():\n",
    "            if word in self.model.wv.vocab:\n",
    "                self.vocabulary.append(word)\n",
    "    \n",
    "    \n",
    "    def __vectorize_corpus(self):\n",
    "        print(\"vectorizing documents...\")\n",
    "        self.vectorizer = CountVectorizer(vocabulary=self.vocabulary)\n",
    "        self.docIDs = []\n",
    "        docs = []\n",
    "        for document in self.corpus.documents:\n",
    "            self.docIDs.append(document.docID)\n",
    "            docs.append(document.content)\n",
    "        self.corpus_vector = []\n",
    "        for v in self.vectorizer.transform(docs):\n",
    "            v = v.toarray().ravel()\n",
    "            v = v.astype(np.double)\n",
    "            v /= v.sum()\n",
    "            self.corpus_vector.append(v)\n",
    "        \n",
    "        \n",
    "    def __calculate_word_distance(self):\n",
    "        W = np.array([self.model[w] for w in self.vectorizer.get_feature_names()\n",
    "                         if w in self.model])\n",
    "        self.words_distance = euclidean_distances(W).astype(np.double)\n",
    "        self.words_distance /= self.words_distance.max()\n",
    "        \n",
    "        \n",
    "    def __documents_score(self, query_vector):\n",
    "        documents_score = Counter()\n",
    "        for index, doc_vector in enumerate(self.corpus_vector):\n",
    "            docID = self.docIDs[index]\n",
    "            vector1_ix = np.nonzero(query_vector)\n",
    "            vector2_ix = np.nonzero(doc_vector)\n",
    "            union_idx = np.union1d(vector1_ix, vector2_ix)\n",
    "            vector1 = query_vector[union_idx]\n",
    "            vector2 = doc_vector[union_idx]\n",
    "            D = self.words_distance[:,union_idx][union_idx]\n",
    "            score = emd.emd(vector1, vector2, D)\n",
    "            documents_score[docID] = score\n",
    "        \n",
    "        documents_score = list(documents_score.items())\n",
    "        documents_score.sort(key=lambda ds: ds[1], reverse=False)\n",
    "        return documents_score\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDAModel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LSHForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAModel:\n",
    "    def __init__(self, corpus, preprocesser, num_topics=20):\n",
    "        self.origin_corpus = corpus\n",
    "        self.preprocessor = preprocessor\n",
    "        self.num_topics = num_topics\n",
    "        self.docID = []\n",
    "        self.dictionary = None\n",
    "        self.corpus = None\n",
    "        \n",
    "        \n",
    "    def build(self):\n",
    "        print(\"build corpus...\")\n",
    "        self.__build_corpus()\n",
    "        print(\"build model...\")\n",
    "        self.__build_model()\n",
    "        print(\"vectorize documents...\")\n",
    "        self.__vectorize_corpus()\n",
    "        \n",
    "    \n",
    "    def search(self, query, topK=20, normalized=False):\n",
    "        query_vector = self.__vectorize(query)\n",
    "        scores = self.__document_scores(query_vector)\n",
    "        return scores\n",
    "        \n",
    "        \n",
    "    def __vectorize(self, text):\n",
    "        tokens = self.preprocessor.process(text)\n",
    "        bow = self.dictionary.doc2bow(tokens)\n",
    "        vector = [x[1] for x in self.model.get_document_topics(bow, \n",
    "                                                               minimum_probability=0.0)]\n",
    "        return vector\n",
    "        \n",
    "        \n",
    "    def __build_corpus(self):\n",
    "        self.texts = []\n",
    "        for document in self.origin_corpus.documents:\n",
    "            docID = document.docID\n",
    "            tokens = self.preprocessor.process(document.content)\n",
    "            self.docID.append(docID)\n",
    "            self.texts.append(tokens)\n",
    "        self.dictionary = corpora.Dictionary(self.texts)\n",
    "        self.corpus = [self.dictionary.doc2bow(text) for text in self.texts]\n",
    "        \n",
    "    \n",
    "    def __build_model(self):\n",
    "        self.model = models.ldamodel.LdaModel(self.corpus, \n",
    "                                              id2word=self.dictionary,\n",
    "                                              num_topics=self.num_topics)\n",
    "    \n",
    "    \n",
    "    def __vectorize_corpus(self):\n",
    "        self.lsh = LSHForest(n_estimators=200, \n",
    "                             n_neighbors=self.num_topics)\n",
    "        self.vectorized_docs = []\n",
    "        for text in self.texts:\n",
    "            bow = self.dictionary.doc2bow(text)\n",
    "            vectorized_doc = [x[1] for x in self.model.get_document_topics(bow, \n",
    "                                                                          minimum_probability=0.0)]\n",
    "            self.vectorized_docs.append(vectorized_doc)\n",
    "        self.lsh.fit(self.vectorized_docs)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def __document_scores(self, query_vector):\n",
    "        distances, indices = self.lsh.kneighbors([query_vector])\n",
    "        document_scores = Counter()\n",
    "        for i, distance in enumerate(distances[0]):\n",
    "            index = indices[0][i]\n",
    "            document_scores[self.docID[index]] = distance\n",
    "        document_scores = list(document_scores.items())\n",
    "        return document_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus_2:\n",
    "    \n",
    "    def __init__(self, extractor):\n",
    "        self.extractor = extractor\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def build(self, path):\n",
    "        self.documents = {}\n",
    "        for document in self.extractor.extract(path):\n",
    "            self.documents[document.docID] = document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_file = 'G:\\\\dataset\\\\corpus\\\\OHSUMED\\\\ohsu-trec\\\\trec9-train\\\\ohsumed.87'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexing content...\n",
      "indexing authors...\n",
      "indexing title...\n",
      "indexing keywords...\n",
      "indexing publication...\n"
     ]
    }
   ],
   "source": [
    "extractor = OHSUMED_Extractor()\n",
    "corpus = Corpus(extractor)\n",
    "corpus.build(document_file)\n",
    "preprocessor = Preprocessor()\n",
    "print('indexing content...')\n",
    "indexer = Indexer(preprocessor, 'content')\n",
    "indexer.index(corpus)\n",
    "print('indexing authors...')\n",
    "indexer_authors = Indexer(preprocessor, 'authors')\n",
    "indexer_authors.index(corpus)\n",
    "print('indexing title...')\n",
    "indexer_title = Indexer(preprocessor, 'title')\n",
    "indexer_title.index(corpus)\n",
    "print('indexing keywords...')\n",
    "indexer_keywords = Indexer(preprocessor, 'keywords')\n",
    "indexer_keywords.index(corpus)\n",
    "print('indexing publication...')\n",
    "indexer_publication = Indexer(preprocessor, 'publication')\n",
    "indexer_publication.index(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_2 = Corpus_2(extractor)\n",
    "corpus_2.build(document_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_model = BooleanModel(indexer)\n",
    "boolean_model_authors = BooleanModel(indexer_authors)\n",
    "boolean_model_title = BooleanModel(indexer_title)\n",
    "boolean_model_keywords = BooleanModel(indexer_keywords)\n",
    "boolean_model_publication = BooleanModel(indexer_publication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building corpus...\n",
      "build corpus...\n",
      "build model...\n",
      "vectorize documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\approximate.py:220: DeprecationWarning: LSHForest has poor performance and has been deprecated in 0.19. It will be removed in version 0.21.\n",
      "  DeprecationWarning)\n",
      "E:\\Anaconda3\\lib\\site-packages\\sklearn\\random_projection.py:378: DataDimensionalityWarning: The number of components is higher than the number of features: n_features < n_components (20 < 32).The dimensionality of the problem will not be reduced.\n",
      "  DataDimensionalityWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocabulary...\n",
      "vectorizing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:57: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "E:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:57: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "tf_model = TFModel(corpus, indexer)\n",
    "args = 'log'\n",
    "tf_idf_model = TF_IDFModel(corpus, indexer, args)\n",
    "ts_ss_model = TS_SSModel(corpus, indexer, preprocessor, measure='TS-SS')\n",
    "lsi_model = LSIModel(corpus, preprocessor, \".\\\\lsi\")\n",
    "lsi_model.load()\n",
    "lda_model = LDAModel(corpus, preprocessor)\n",
    "lda_model.build()\n",
    "wmd_model = WMDModel(corpus, indexer, preprocessor)\n",
    "wmd_model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(model, query, topK=20):\n",
    "    result = model.search(query, topK=topK, normalized=True)\n",
    "    \n",
    "    document_tuple = []\n",
    "    for docID, similarity in result:\n",
    "        content = corpus_2.documents[docID]\n",
    "        document_tuple.append((docID, similarity, content))\n",
    "    return document_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show(document_tuples, query, field='content'):\n",
    "    i=1\n",
    "    for docID, similarity, document in document_tuples:\n",
    "        print('['+ str(i) + '] docID: \\033[34m%d\\t\\033[0msimilarity/distance: \\033[32m%.5f\\033[0m' \n",
    "              % (int(docID), float(similarity)))\n",
    "        content = document.content\n",
    "        title = document.title\n",
    "        keywords = '; '.join(document.keyswords)\n",
    "        authors = '; '.join(document.authors)\n",
    "        publication = ' '.join(document.publication)\n",
    "        if field == 'content':\n",
    "            content = mark(content, query)\n",
    "        elif field == 'title':\n",
    "            title = mark(title, query)\n",
    "        elif field == 'keywords':\n",
    "            keywords = mark(keywords, query)\n",
    "        elif field == 'authors':\n",
    "            authors = mark(authors, query)\n",
    "        elif field == 'publication':\n",
    "            publication = mark(publication, query)\n",
    "        print('    Title: \\033[4m%s\\033[0m\\n' % title)\n",
    "        print('    Authors: ' + authors)\n",
    "        print('    Keywords: ' + keywords)\n",
    "        print('    Abstract: ' + content)\n",
    "        print('    Publication: ' + publication)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_similarity(word1, word2):\n",
    "    set1 = set(word1)\n",
    "    set2 = set(word2)\n",
    "    jaccord = len(set.intersection(set1, set2)) / (len(set.union(set1, set2)))\n",
    "    if jaccord > 0.8:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark(content, query):\n",
    "    result = ''\n",
    "    for word1 in content.split():\n",
    "        tag = False\n",
    "        for word2 in query.split():\n",
    "            if is_similarity(word1, word2):\n",
    "                tag = True\n",
    "        if tag:\n",
    "            result += \"\\033[1;31m\"+ word1 + \"\\033[0m \"\n",
    "        else:\n",
    "            result += word1 + \" \"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_document(query, field='content', model_type='boolean', topK=20):\n",
    "    if field == 'content':\n",
    "        if model_type == 'boolean':\n",
    "            model = boolean_model\n",
    "        elif model_type == 'tf':\n",
    "            model = tf_model\n",
    "        elif model_type == 'tf-idf':\n",
    "            model = tf_idf_model\n",
    "        elif model_type == 'ts-ss':\n",
    "            model = ts_ss_model\n",
    "        elif model_type == 'lsi':\n",
    "            model = lsi_model\n",
    "        elif model_type == 'lda':\n",
    "            model = lda_model\n",
    "        elif model_type == 'wmd':\n",
    "            model = wmd_model\n",
    "    elif field == 'authors':\n",
    "        model = boolean_model_authors\n",
    "    elif field == 'keywords':\n",
    "        model = boolean_model_keywords\n",
    "    elif field == 'title':\n",
    "        model = boolean_model_title\n",
    "    elif field == 'publication':\n",
    "        model = boolean_model_publication\n",
    "    \n",
    "    show(search(model, query, topK), query, field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \" Although certain gold [Au(I)] compounds have been used effectively in the treatment of rheumatoid arthritis for some years, the molecular basis for such therapeutic action has been unclear. One possible mechanism of the action of Au(I) compounds is that they\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] docID: \u001b[34m87177966\t\u001b[0msimilarity/distance: \u001b[32m0.15969\u001b[0m\n",
      "    Title: \u001b[4mAntiarthritic gold compounds effectively quench electronically excited singlet oxygen.\u001b[0m\n",
      "\n",
      "    Authors: Corey EJ; Mehrotra MM; Khan AU\n",
      "    Keywords: Arthritis, Rheumatoid; Auranofin; Chemistry, Physical; Human; Kinetics; Lipid Peroxides; Oxygen; Support, Non-U.S. Gov't; Support, U.S. Gov't, P.H.S\n",
      "    Abstract: \u001b[1;31mAlthough\u001b[0m \u001b[1;31mcertain\u001b[0m \u001b[1;31mgold\u001b[0m \u001b[1;31m[Au(I)]\u001b[0m \u001b[1;31mcompounds\u001b[0m \u001b[1;31mhave\u001b[0m \u001b[1;31mbeen\u001b[0m \u001b[1;31mused\u001b[0m \u001b[1;31meffectively\u001b[0m \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m \u001b[1;31mtreatment\u001b[0m \u001b[1;31mof\u001b[0m \u001b[1;31mrheumatoid\u001b[0m \u001b[1;31marthritis\u001b[0m \u001b[1;31mfor\u001b[0m \u001b[1;31msome\u001b[0m \u001b[1;31myears,\u001b[0m \u001b[1;31mthe\u001b[0m \u001b[1;31mmolecular\u001b[0m \u001b[1;31mbasis\u001b[0m \u001b[1;31mfor\u001b[0m \u001b[1;31msuch\u001b[0m \u001b[1;31mtherapeutic\u001b[0m \u001b[1;31maction\u001b[0m \u001b[1;31mhas\u001b[0m \u001b[1;31mbeen\u001b[0m \u001b[1;31munclear.\u001b[0m \u001b[1;31mOne\u001b[0m \u001b[1;31mpossible\u001b[0m \u001b[1;31mmechanism\u001b[0m \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m \u001b[1;31maction\u001b[0m \u001b[1;31mof\u001b[0m \u001b[1;31mAu(I)\u001b[0m \u001b[1;31mcompounds\u001b[0m \u001b[1;31mis\u001b[0m \u001b[1;31mthat\u001b[0m \u001b[1;31mthey\u001b[0m protect unsaturated membrane lipids and proteins against oxidative degradation caused by activated phagocytes \u001b[1;31mthat\u001b[0m are not properly regulated. In this study it \u001b[1;31mhas\u001b[0m \u001b[1;31mbeen\u001b[0m shown \u001b[1;31mthat\u001b[0m superoxide ion (O-2.), a product \u001b[1;31mof\u001b[0m activated phagocytes, can be oxidized to electronically excited singlet oxygen (O1(2)delta g), an agent \u001b[1;31mthat\u001b[0m \u001b[1;31mis\u001b[0m capable \u001b[1;31mof\u001b[0m peroxidation \u001b[1;31mof\u001b[0m unsaturated fatty acid derivatives. It \u001b[1;31mhas\u001b[0m also \u001b[1;31mbeen\u001b[0m shown \u001b[1;31mthat\u001b[0m antiarthritic \u001b[1;31mAu(I)\u001b[0m \u001b[1;31mcompounds\u001b[0m are effective deactivators \u001b[1;31mof\u001b[0m O1(2)delta g with quenching constants on \u001b[1;31mthe\u001b[0m order \u001b[1;31mof\u001b[0m 10(7) M-1 sec-1. \n",
      "    Publication: Science 8707; 236(4797):68-9\n",
      "[2] docID: \u001b[34m87124496\t\u001b[0msimilarity/distance: \u001b[32m0.21166\u001b[0m\n",
      "    Title: \u001b[4mExperience with calcium antagonist drugs in congestive heart failure.\u001b[0m\n",
      "\n",
      "    Authors: O'Rourke RA; Walsh RA\n",
      "    Keywords: Calcium Channel Blockers; Comparative Study; Heart Failure, Congestive; Hemodynamics; Human; Nifedipine; Vasodilation; Vasodilator Agents; Verapamil\n",
      "    Abstract: Several clinical studies \u001b[1;31mhave\u001b[0m demonstrated beneficial hemodynamic effects \u001b[1;31mof\u001b[0m calcium antagonist drugs when \u001b[1;31mused\u001b[0m as arterial vasodilators \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m \u001b[1;31mtreatment\u001b[0m \u001b[1;31mof\u001b[0m \u001b[1;31mcertain\u001b[0m patients with moderate to severe congestive heart failure. These drugs usually decrease systemic vascular \u001b[1;31mresistance\u001b[0m and improve ejection phase indexes \u001b[1;31mof\u001b[0m left ventricular function \u001b[1;31min\u001b[0m \u001b[1;31msuch\u001b[0m patients. However, calcium antagonists \u001b[1;31mhave\u001b[0m intrinsic negative inotropic effects and other vasodilators \u001b[1;31msuch\u001b[0m as nitroprusside, hydralazine and captopril appear to be more beneficial when \u001b[1;31mused\u001b[0m \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m \u001b[1;31mtreatment\u001b[0m \u001b[1;31mof\u001b[0m severe congestive heart failure. \n",
      "    Publication: Am J Cardiol 8705; 59(3):64B-69B\n",
      "[3] docID: \u001b[34m87263578\t\u001b[0msimilarity/distance: \u001b[32m0.21696\u001b[0m\n",
      "    Title: \u001b[4mSuccessful percutaneous drainage of pyogenic liver abscess complicated by bronchobiliary fistula.\u001b[0m\n",
      "\n",
      "    Authors: Bilfinger TV; Oldham KT; Lobe TE; Barron S; Hayden CK\n",
      "    Keywords: Acute Disease; Antibiotics; Biliary Fistula; Bronchial Fistula; Case Report; Child; Combined Modality Therapy; Drainage; Female; Human; Liver Abscess; Lung Abscess; Suppuration\n",
      "    Abstract: We \u001b[1;31mhave\u001b[0m \u001b[1;31mused\u001b[0m percutaneous transhepatic drainage \u001b[1;31mof\u001b[0m pyogenic hepatic abscesses with good result. This technique \u001b[1;31mhas\u001b[0m \u001b[1;31mbeen\u001b[0m \u001b[1;31mused\u001b[0m \u001b[1;31meffectively\u001b[0m \u001b[1;31min\u001b[0m adults by several authors. We believe this report represents \u001b[1;31mthe\u001b[0m first instance \u001b[1;31mof\u001b[0m successful percutaneous drainage \u001b[1;31mof\u001b[0m a pyogenic liver abscess complicated by a bronchobiliary fistula. \n",
      "    Publication: South Med J 8710; 80(7):907-9\n",
      "[4] docID: \u001b[34m87195567\t\u001b[0msimilarity/distance: \u001b[32m0.21905\u001b[0m\n",
      "    Title: \u001b[4mGold therapy and its indications in dermatology. A review.\u001b[0m\n",
      "\n",
      "    Authors: Thomas I\n",
      "    Keywords: Gold; Human; Pemphigus; Psoriasis\n",
      "    Abstract: Gold \u001b[1;31mcompounds\u001b[0m \u001b[1;31mhave\u001b[0m long \u001b[1;31mbeen\u001b[0m known as \u001b[1;31mtherapeutic\u001b[0m agents and \u001b[1;31mhave\u001b[0m \u001b[1;31mbeen\u001b[0m \u001b[1;31mused\u001b[0m extensively \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m \u001b[1;31mtreatment\u001b[0m \u001b[1;31mof\u001b[0m \u001b[1;31mrheumatoid\u001b[0m \u001b[1;31marthritis.\u001b[0m Their \u001b[1;31mmechanisms\u001b[0m \u001b[1;31mof\u001b[0m \u001b[1;31maction\u001b[0m \u001b[1;31min\u001b[0m vivo, however, remain \u001b[1;31munclear.\u001b[0m In comparison to parenteral gold, \u001b[1;31mthe\u001b[0m pharmacokinetics \u001b[1;31mof\u001b[0m a newly available oral compound, auranofin, differ greatly. Auranofin also appears to \u001b[1;31mhave\u001b[0m specific immunomodulatory \u001b[1;31mactions\u001b[0m and to be associated with fewer and milder toxic effects. The role \u001b[1;31mof\u001b[0m chrysotherapy \u001b[1;31min\u001b[0m dermatology \u001b[1;31mhas\u001b[0m \u001b[1;31mbeen\u001b[0m reemphasized recently. It may be \u001b[1;31mused\u001b[0m as an adjuvant \u001b[1;31min\u001b[0m pemphigus or other steroid-responsive diseases to help control disease activity and to taper or eliminate corticosteroid therapy more rapidly. Reports on \u001b[1;31mthe\u001b[0m use \u001b[1;31mof\u001b[0m \u001b[1;31mgold\u001b[0m \u001b[1;31min\u001b[0m dermatology are otherwise limited. They include one case \u001b[1;31mof\u001b[0m epidermolysis bullosa acquisita and psoriatic \u001b[1;31marthritis.\u001b[0m The benefits \u001b[1;31mof\u001b[0m \u001b[1;31mgold\u001b[0m \u001b[1;31mtreatment\u001b[0m \u001b[1;31mhave\u001b[0m to be weighed against \u001b[1;31mthe\u001b[0m risks inherent \u001b[1;31min\u001b[0m its adverse reactions, \u001b[1;31msome\u001b[0m \u001b[1;31mof\u001b[0m which are serious and unpredictable. \n",
      "    Publication: J Am Acad Dermatol 8708; 16(4):845-54\n",
      "[5] docID: \u001b[34m87234821\t\u001b[0msimilarity/distance: \u001b[32m0.21921\u001b[0m\n",
      "    Title: \u001b[4mThe use of antibiotics in surgical treatment of the colon.\u001b[0m\n",
      "\n",
      "    Authors: Menaker GJ\n",
      "    Keywords: Antibiotics; Clinical Trials; Colon and Rectal Surgery (Specialty); Human; Intestines; Premedication; Sulfonamides\n",
      "    Abstract: The judicious use \u001b[1;31mof\u001b[0m antibiotics seems to be indicated \u001b[1;31min\u001b[0m operations upon \u001b[1;31mthe\u001b[0m colon, although these procedures were performed with similar morbidity rates \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m 1930's without \u001b[1;31mthe\u001b[0m use \u001b[1;31mof\u001b[0m antibiotics. The results \u001b[1;31mof\u001b[0m recent studies indicate \u001b[1;31mthat\u001b[0m systemic antibiotics administered preoperatively and \u001b[1;31mfor\u001b[0m a short perioperative interval \u001b[1;31mis\u001b[0m \u001b[1;31mthe\u001b[0m preferred method \u001b[1;31mof\u001b[0m \u001b[1;31mtreatment\u001b[0m because it \u001b[1;31mhas\u001b[0m little effect on intestinal colonization. Surgical principles \u001b[1;31mhave\u001b[0m not materially changed over \u001b[1;31mthe\u001b[0m \u001b[1;31myears\u001b[0m and antibiotics are not indicated merely to cover breaks \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m operative technique. \u001b[1;31mOne\u001b[0m must always be cautious \u001b[1;31mof\u001b[0m \u001b[1;31mpossible\u001b[0m untoward reactions and complications. This may be another example \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m principle--less \u001b[1;31mis\u001b[0m more. \n",
      "    Publication: Surg Gynecol Obstet 8709; 164(6):581-6\n",
      "[6] docID: \u001b[34m87171985\t\u001b[0msimilarity/distance: \u001b[32m0.22093\u001b[0m\n",
      "    Title: \u001b[4mTreatment of hemifacial spasm with transcutaneous electrical stimulation.\u001b[0m\n",
      "\n",
      "    Authors: Yamamoto E; Nishimura H\n",
      "    Keywords: Adult; Aged; Electric Stimulation Therapy; Facial Muscles; Female; Human; Male; Middle Age; Spasm; Transcutaneous Electric Nerve Stimulation\n",
      "    Abstract: A transcutaneous electrical stimulation method was \u001b[1;31mused\u001b[0m to treat hemifacial spasms. This \u001b[1;31mtreatment\u001b[0m was applied concomitantly with drug therapy to 21 patients, and was found effective \u001b[1;31min\u001b[0m 17 patients (81%). Electrotherapy \u001b[1;31mis\u001b[0m considered to be useful \u001b[1;31min\u001b[0m patients with mild spasms and \u001b[1;31min\u001b[0m those with severe spasms \u001b[1;31mfor\u001b[0m whom surgical \u001b[1;31mtreatment\u001b[0m \u001b[1;31mis\u001b[0m not \u001b[1;31mpossible.\u001b[0m \n",
      "    Publication: Laryngoscope 8707; 97(4):458-60\n",
      "[7] docID: \u001b[34m87311537\t\u001b[0msimilarity/distance: \u001b[32m0.22242\u001b[0m\n",
      "    Title: \u001b[4mNonsteroidal antiinflammatory drugs and articular cartilage.\u001b[0m\n",
      "\n",
      "    Authors: Brandt KD\n",
      "    Keywords: Animal; Anti-Inflammatory Agents, Non-Steroidal; Aspirin; Cartilage, Articular; Dogs; In Vitro; Osteoarthritis; Proteoglycans; Support, Non-U.S. Gov't; Support, U.S. Gov't, P.H.S\n",
      "    Abstract: Salicylates and \u001b[1;31msome\u001b[0m other nonsteroidal antiinflammatory drugs (NSAID) suppress proteoglycan biosynthesis \u001b[1;31min\u001b[0m normal articular cartilage \u001b[1;31min\u001b[0m vitro. Their effect on osteoarthritic cartilage \u001b[1;31min\u001b[0m vitro \u001b[1;31mis\u001b[0m even greater than their effect on normal cartilage. Aspirin \u001b[1;31mhas\u001b[0m a similar effect \u001b[1;31min\u001b[0m vivo on both atrophic cartilage and osteoarthritic cartilage \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m dog, although no \u001b[1;31min\u001b[0m vivo effect \u001b[1;31mof\u001b[0m salicylate on normal joint cartilage \u001b[1;31mhas\u001b[0m \u001b[1;31mbeen\u001b[0m observed. While \u001b[1;31mthe\u001b[0m magnitude \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m effects \u001b[1;31mof\u001b[0m NSAID on proteoglycan metabolism \u001b[1;31min\u001b[0m cartilage appeared to be inversely related to \u001b[1;31mthe\u001b[0m proteoglycan content \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m matrix, it \u001b[1;31mis\u001b[0m \u001b[1;31mpossible\u001b[0m \u001b[1;31mthat\u001b[0m \u001b[1;31msome\u001b[0m drugs are selectively bound to cartilage matrix components, which could affect their \u001b[1;31maction\u001b[0m on \u001b[1;31mthe\u001b[0m chondrocyte. If NSAID \u001b[1;31mhave\u001b[0m similar effects \u001b[1;31min\u001b[0m patients with \u001b[1;31marthritis,\u001b[0m this could \u001b[1;31mhave\u001b[0m implications with respect to articular cartilage lesions. \n",
      "    Publication: J Rheumatol 8712; 14 Spec No:132-3\n",
      "[8] docID: \u001b[34m87325518\t\u001b[0msimilarity/distance: \u001b[32m0.22268\u001b[0m\n",
      "    Title: \u001b[4mChondroid chordoma of the sacrococcygeal region.\u001b[0m\n",
      "\n",
      "    Authors: Chu TA\n",
      "    Keywords: Bone Neoplasms; Case Report; Chordoma; Human; Immunoenzyme Techniques; Keratin; Male; Middle Age; Nerve Tissue Protein S 100; Sacrococcygeal Region\n",
      "    Abstract: This \u001b[1;31mis\u001b[0m a unique case \u001b[1;31mof\u001b[0m chondroid chordoma arising \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m sacrococcygeal area \u001b[1;31mof\u001b[0m an asymptomatic man. S100 protein was detected \u001b[1;31min\u001b[0m both chondroid and chordoid tissues; epithelial membrane antigen (EMA) and cytokeratin were present \u001b[1;31min\u001b[0m abundant amounts \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m cytoplasm \u001b[1;31mof\u001b[0m chordoma cells but not \u001b[1;31min\u001b[0m chondroid cells. The presence \u001b[1;31mof\u001b[0m cytokeratin and EMA \u001b[1;31min\u001b[0m chordoma implies \u001b[1;31mthe\u001b[0m epithelial nature \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m tumor and \u001b[1;31mis\u001b[0m extremely helpful as a differential marker \u001b[1;31mfor\u001b[0m chordoma. I suggest \u001b[1;31mthat\u001b[0m EMA, cytokeratin, and S100 protein should be \u001b[1;31mused\u001b[0m conjunctively to provide a resolution \u001b[1;31mfor\u001b[0m difficult diagnostic problems when dealing with chordomas, especially with their variants, \u001b[1;31msuch\u001b[0m as chondroid chordomas, which often cause further confusion. \n",
      "    Publication: Arch Pathol Lab Med 8712; 111(9):861-4\n",
      "[9] docID: \u001b[34m87127178\t\u001b[0msimilarity/distance: \u001b[32m0.22271\u001b[0m\n",
      "    Title: \u001b[4mEfficacy and safety of aztreonam in the treatment of serious gram-negative bacterial infections.\u001b[0m\n",
      "\n",
      "    Authors: LeFrock JL; Smith BR; Chandrasekar P; Rolston KV; Molavi A; Kannangara W\n",
      "    Keywords: Adult; Aged; Aged, 80 and over; Aztreonam; Bacterial Infections; Female; Gram-Negative Bacteria; Human; Male; Middle Age; Respiratory Tract Infections; Septicemia; Skin Diseases, Infectious; Urinary Tract Infections\n",
      "    Abstract: Aztreonam was \u001b[1;31mused\u001b[0m \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m initial \u001b[1;31mtreatment\u001b[0m \u001b[1;31mof\u001b[0m infection \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m urinary tract (23 cases), respiratory tract (17 cases), skin and soft tissue (12 cases), abdominal cavity (three cases), endocarditis (two cases), septicemia (eight cases), and osteomyelitis (two cases). In 26 \u001b[1;31mof\u001b[0m 60 evaluable infectious episodes, aztreonam was \u001b[1;31mused\u001b[0m alone. Clinical cure was observed \u001b[1;31min\u001b[0m 35 \u001b[1;31mof\u001b[0m 60, improvement \u001b[1;31min\u001b[0m 24 \u001b[1;31mof\u001b[0m 60, and failure \u001b[1;31min\u001b[0m one \u001b[1;31mof\u001b[0m 60 cases. Ten patients developed subsequent superinfection. Aztreonam was well tolerated, although one case \u001b[1;31mof\u001b[0m exfoliative dermatitis and one \u001b[1;31mof\u001b[0m pseudomembranous colitis occurred. However, these cases were complicated by proximal administration \u001b[1;31mof\u001b[0m other antibiotics. \n",
      "    Publication: Arch Intern Med 8705; 147(2):325-8\n",
      "[10] docID: \u001b[34m87140294\t\u001b[0msimilarity/distance: \u001b[32m0.22303\u001b[0m\n",
      "    Title: \u001b[4mRadionuclide diagnosis and therapy of neural crest tumors using iodine-131 metaiodobenzylguanidine.\u001b[0m\n",
      "\n",
      "    Authors: Hoefnagel CA; Voute PA; de Kraker J; Marcuse HR\n",
      "    Keywords: Adolescence; Adult; Child; Child, Preschool; Evaluation Studies; Female; Human; Infant; Infant, Newborn; Iodine Radioisotopes; Iodobenzenes; Male; Middle Age; Neoplasm Metastasis; Neoplasms; Neural Crest; Neuroblastoma; Radiotherapy Dosage; Time Factors; Whole-Body Counting\n",
      "    Abstract: The successful application \u001b[1;31mof\u001b[0m [131I]metaiodobenzylguanidine (MIBG) \u001b[1;31min\u001b[0m diagnosis and therapy \u001b[1;31mof\u001b[0m pheochromocytoma \u001b[1;31mhas\u001b[0m led to its use \u001b[1;31min\u001b[0m other tumors which derive from \u001b[1;31mthe\u001b[0m neural crest and potentially concentrate this radiopharmaceutical as well. In \u001b[1;31mthe\u001b[0m present series, [131]MIBG total-body scintigraphy was \u001b[1;31mused\u001b[0m \u001b[1;31mfor\u001b[0m detection \u001b[1;31mof\u001b[0m neuroblastoma \u001b[1;31min\u001b[0m 47 patients and 47 cases \u001b[1;31mof\u001b[0m other neural crest tumors. The method was found to be as reliable \u001b[1;31min\u001b[0m neuroblastoma (sensitivity 95%, specificity 100%), as it \u001b[1;31mis\u001b[0m \u001b[1;31min\u001b[0m pheochromocytoma. \u001b[1;31mAlthough\u001b[0m other neural crest tumors may concentrate [131I]MIBG, this \u001b[1;31mis\u001b[0m not a consistent finding; however, it \u001b[1;31mis\u001b[0m useful to investigate which tumors do, as this may provide an alternative \u001b[1;31mtreatment\u001b[0m modality \u001b[1;31mfor\u001b[0m \u001b[1;31msome\u001b[0m patients. \u001b[1;31mAlthough\u001b[0m followup \u001b[1;31mis\u001b[0m still very short, preliminary results \u001b[1;31mof\u001b[0m \u001b[1;31mtherapeutic\u001b[0m use \u001b[1;31mof\u001b[0m [131I] MIBG \u001b[1;31min\u001b[0m 21 patients indicate \u001b[1;31mthat\u001b[0m this \u001b[1;31mtreatment\u001b[0m modality may be effective \u001b[1;31min\u001b[0m neuroblastoma and malignant pheochromocytoma. \n",
      "    Publication: J Nucl Med 8706; 28(3):308-14\n",
      "[11] docID: \u001b[34m87178023\t\u001b[0msimilarity/distance: \u001b[32m0.22552\u001b[0m\n",
      "    Title: \u001b[4mA small gold-conjugated antibody label: improved resolution for electron microscopy.\u001b[0m\n",
      "\n",
      "    Authors: Hainfeld JF\n",
      "    Keywords: Ferritin; Gold; Immunoglobulins, Fab; Microscopy, Electron; Support, U.S. Gov't, Non-P.H.S.; Support, U.S. Gov't, P.H.S\n",
      "    Abstract: A general method \u001b[1;31mhas\u001b[0m \u001b[1;31mbeen\u001b[0m developed to make \u001b[1;31mthe\u001b[0m smallest gold-conjugated antibody label yet developed \u001b[1;31mfor\u001b[0m electron microscopy. It should \u001b[1;31mhave\u001b[0m wide application \u001b[1;31min\u001b[0m domainal mapping \u001b[1;31mof\u001b[0m single molecules or \u001b[1;31min\u001b[0m pinpointing specific molecules, sites, or sequences \u001b[1;31min\u001b[0m supramolecular complexes. It permits electron microscopic visualization \u001b[1;31mof\u001b[0m single antigen-binding antibody fragments (Fab') by covalently linking an 11-atom \u001b[1;31mgold\u001b[0m cluster to a specific residue on each Fab' \u001b[1;31msuch\u001b[0m \u001b[1;31mthat\u001b[0m \u001b[1;31mthe\u001b[0m antigenic specificity and capacity are preserved. The distance \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m \u001b[1;31mgold\u001b[0m cluster from \u001b[1;31mthe\u001b[0m antigen \u001b[1;31mis\u001b[0m a maximum \u001b[1;31mof\u001b[0m 5.0 nanometers when \u001b[1;31mthe\u001b[0m undecagold-Fab' probe \u001b[1;31mis\u001b[0m \u001b[1;31mused\u001b[0m as an immunolabel. \n",
      "    Publication: Science 8707; 236(4800):450-3\n",
      "[12] docID: \u001b[34m87254063\t\u001b[0msimilarity/distance: \u001b[32m0.22578\u001b[0m\n",
      "    Title: \u001b[4mUnusual features of parovarian cysts. A report of two cases.\u001b[0m\n",
      "\n",
      "    Authors: Reuter KL; Meyer RN\n",
      "    Keywords: Adnexa Uteri; Adolescence; Adult; Case Report; Cystadenoma; Female; Genital Neoplasms, Female; Human; Parovarian Cyst; Ultrasonography; Uterus\n",
      "    Abstract: In two cases, parovarian cysts were well characterized by ultrasonography and presented with unusual clinical features. In one case \u001b[1;31mthe\u001b[0m cysts were present bilaterally and associated with an arcuate uterus. In \u001b[1;31mthe\u001b[0m other, histology \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m cyst revealed a papillary serous cystadenoma. The lesion was also associated with an arcuate uterus. \n",
      "    Publication: J Reprod Med 8710; 32(5):371-4\n",
      "[13] docID: \u001b[34m87178174\t\u001b[0msimilarity/distance: \u001b[32m0.22625\u001b[0m\n",
      "    Title: \u001b[4mPharmacologic erection program for the treatment of male impotence.\u001b[0m\n",
      "\n",
      "    Authors: Trapp JD\n",
      "    Keywords: Drug Therapy, Combination; Human; Impotence; Injections; Male; Papaverine; Penile Erection; Penis; Phentolamine; Self Administration\n",
      "    Abstract: Recent advances \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m understanding \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m physiology \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m erection process \u001b[1;31mhave\u001b[0m led to exciting new diagnostic and \u001b[1;31mtherapeutic\u001b[0m techniques \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m management \u001b[1;31mof\u001b[0m impotence. Intracavernous injection \u001b[1;31mof\u001b[0m pharmacologic agents can be \u001b[1;31mused\u001b[0m to both diagnose and treat \u001b[1;31mcertain\u001b[0m types \u001b[1;31mof\u001b[0m impotence. At \u001b[1;31mthe\u001b[0m Nashville Impotence Center we use intracavernous injections \u001b[1;31mof\u001b[0m papaverine-phentolamine to diagnose vascular impotence and to treat \u001b[1;31mcertain\u001b[0m patients with organic impotence as well as \u001b[1;31msome\u001b[0m patients with psychologic impotence. \u001b[1;31mOne\u001b[0m hundred thirty-six patients are currently enrolled \u001b[1;31min\u001b[0m our self-injection \u001b[1;31mtreatment\u001b[0m program. Complications and side effects \u001b[1;31mhave\u001b[0m \u001b[1;31mbeen\u001b[0m rare. \n",
      "    Publication: South Med J 8707; 80(4):426-7\n",
      "[14] docID: \u001b[34m87202873\t\u001b[0msimilarity/distance: \u001b[32m0.22649\u001b[0m\n",
      "    Title: \u001b[4mEyelid necrosis following intralesional corticosteroid injection for capillary hemangioma.\u001b[0m\n",
      "\n",
      "    Authors: Sutula FC; Glover AT\n",
      "    Keywords: Betamethasone; Case Report; Eyelid Neoplasms; Eyelids; Female; Hemangioma; Human; Infant; Necrosis; Triamcinolone\n",
      "    Abstract: The intralesional injection \u001b[1;31mof\u001b[0m corticosteroids \u001b[1;31mhas\u001b[0m \u001b[1;31mbeen\u001b[0m employed successfully \u001b[1;31min\u001b[0m \u001b[1;31mtreatment\u001b[0m \u001b[1;31mof\u001b[0m adnexal neonatal hemangiomas since 1979. This form \u001b[1;31mof\u001b[0m \u001b[1;31mtreatment\u001b[0m \u001b[1;31mis\u001b[0m easily administered, \u001b[1;31mis\u001b[0m repeatable and free from serious complications. We present an exceptional case \u001b[1;31min\u001b[0m which full-thickness eyelid necrosis ensued following intralesional injection \u001b[1;31mof\u001b[0m corticosteroids \u001b[1;31min\u001b[0m a capillary hemangioma. After eyelid reconstruction \u001b[1;31mthe\u001b[0m patient's visual axis \u001b[1;31mhas\u001b[0m remained unobstructed, and amblyopia \u001b[1;31mhas\u001b[0m \u001b[1;31mbeen\u001b[0m thus far averted. \n",
      "    Publication: Ophthalmic Surg 8708; 18(2):103-5\n",
      "[15] docID: \u001b[34m87125612\t\u001b[0msimilarity/distance: \u001b[32m0.22684\u001b[0m\n",
      "    Title: \u001b[4mMechanisms of vitamin deficiencies in alcoholism.\u001b[0m\n",
      "\n",
      "    Authors: Hoyumpa AM\n",
      "    Keywords: Alcoholism; Avitaminosis; Folic Acid Deficiency; Human; Intestinal Absorption; Liver; Pyridoxine Deficiency; Support, U.S. Gov't, Non-P.H.S.; Thiamine Deficiency; Vitamin A Deficiency\n",
      "    Abstract: Chronic alcoholic patients are frequently deficient \u001b[1;31min\u001b[0m one or more vitamins. The deficiencies commonly involve folate, vitamin B6, thiamine, and vitamin A. \u001b[1;31mAlthough\u001b[0m inadequate dietary intake \u001b[1;31mis\u001b[0m a major cause \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m vitamin deficiency, other \u001b[1;31mpossible\u001b[0m \u001b[1;31mmechanisms\u001b[0m may also be involved. Alcoholism can affect \u001b[1;31mthe\u001b[0m absorption, storage, metabolism, and \u001b[1;31mactivation\u001b[0m \u001b[1;31mof\u001b[0m many \u001b[1;31mof\u001b[0m these vitamins. Possible factors which cause alterations \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m absorption, storage, and metabolism \u001b[1;31mof\u001b[0m these vitamins are discussed. Suggestions \u001b[1;31mfor\u001b[0m management \u001b[1;31mof\u001b[0m vitamin deficiencies \u001b[1;31min\u001b[0m chronic alcoholics are also discussed. \n",
      "    Publication: Alcohol Clin Exp Res 8705; 10(6):573-81\n",
      "[16] docID: \u001b[34m87182511\t\u001b[0msimilarity/distance: \u001b[32m0.22742\u001b[0m\n",
      "    Title: \u001b[4mElectromagnetic flowmetry in bypass shunt during carotid endarterectomy.\u001b[0m\n",
      "\n",
      "    Authors: Owen WS\n",
      "    Keywords: Carotid Artery, Internal; Electromagnetics; Endarterectomy; Flowmeters; Heart, Mechanical; Human\n",
      "    Abstract: By use \u001b[1;31mof\u001b[0m a custom-designed electromagnetic flow probe, continuous measurement \u001b[1;31mof\u001b[0m flow \u001b[1;31min\u001b[0m a bypass shunt system \u001b[1;31mis\u001b[0m \u001b[1;31mpossible.\u001b[0m This permits real-time evaluation \u001b[1;31mof\u001b[0m shunt function during carotid endarterectomy and will allow evaluation \u001b[1;31mof\u001b[0m changes made \u001b[1;31min\u001b[0m future shunt designs. \u001b[1;31mAlthough\u001b[0m limited to \u001b[1;31mthe\u001b[0m carotid circulation \u001b[1;31min\u001b[0m this study, \u001b[1;31mthe\u001b[0m method \u001b[1;31mhas\u001b[0m applicability to any vascular region where bypass shunts are used. \n",
      "    Publication: Angiology 8707; 38(3):253-5\n",
      "[17] docID: \u001b[34m87297161\t\u001b[0msimilarity/distance: \u001b[32m0.22795\u001b[0m\n",
      "    Title: \u001b[4mHistologic changes in porcine eyes treated with high-intensity focused ultrasound.\u001b[0m\n",
      "\n",
      "    Authors: Burgess SE; Iwamoto T; Coleman DJ; Lizzi FL; Driller J; Rosado A\n",
      "    Keywords: Animal; Ciliary Body; Conjunctiva; Microscopy, Electron; Necrosis; Sclera; Support, Non-U.S. Gov't; Support, U.S. Gov't, P.H.S.; Swine; Ultrasonic Therapy\n",
      "    Abstract: High-intensity focused ultrasound (HIFU) \u001b[1;31mhas\u001b[0m recently \u001b[1;31mbeen\u001b[0m described \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m \u001b[1;31mtreatment\u001b[0m \u001b[1;31mof\u001b[0m patients with glaucoma. However, despite previous studies \u001b[1;31min\u001b[0m animal and mathematical models, \u001b[1;31mthe\u001b[0m \u001b[1;31mmechanisms\u001b[0m \u001b[1;31mof\u001b[0m \u001b[1;31maction\u001b[0m are not completely understood. We therefore undertook a histologic study \u001b[1;31mof\u001b[0m 15 porcine eyes to evaluate \u001b[1;31mthe\u001b[0m effect \u001b[1;31mof\u001b[0m HIFU and, \u001b[1;31min\u001b[0m particular, \u001b[1;31mthe\u001b[0m changes seen after sequential insonification on previously treated areas \u001b[1;31mof\u001b[0m sclera. We demonstrated initial scleral swelling \u001b[1;31mthat\u001b[0m was followed by scleral thinning. The scleral thinning could be maximized by a second superimposed course \u001b[1;31mof\u001b[0m HIFU. The most consistent change \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m ciliary body was necrosis \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m pars plana. Some treated animals developed a hemorrhagic retinal detachment \u001b[1;31mthat\u001b[0m we believe to be due to \u001b[1;31mthe\u001b[0m presence \u001b[1;31mof\u001b[0m a circumferential blood vessel, which runs \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m ora serrata \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m pig but which \u001b[1;31mis\u001b[0m not present \u001b[1;31min\u001b[0m humans. Based on our observations, \u001b[1;31mpossible\u001b[0m \u001b[1;31mmechanisms\u001b[0m \u001b[1;31mof\u001b[0m \u001b[1;31maction\u001b[0m \u001b[1;31mof\u001b[0m this \u001b[1;31mtreatment\u001b[0m are discussed. \n",
      "    Publication: Ann Ophthalmol 8711; 19(4):133-8\n",
      "[18] docID: \u001b[34m87170010\t\u001b[0msimilarity/distance: \u001b[32m0.22815\u001b[0m\n",
      "    Title: \u001b[4mBilateral carotid body tumors managed with preoperative embolization: a case report and review.\u001b[0m\n",
      "\n",
      "    Authors: DuBois J; Kelly W; McMenamin P; Macbeth GA\n",
      "    Keywords: Adult; Carotid Body Tumor; Case Report; Embolization, Therapeutic; Human; Male; Polyvinyls; Preoperative Care\n",
      "    Abstract: A patient with large bilateral carotid body tumors had preoperative, superselective embolization \u001b[1;31mof\u001b[0m major arterial afferent vessels. After marked reduction \u001b[1;31min\u001b[0m tumor vascularity, total surgical extirpation was then \u001b[1;31mpossible\u001b[0m without significant morbidity or carotid sacrifice. The use \u001b[1;31mof\u001b[0m preoperative embolization \u001b[1;31min\u001b[0m \u001b[1;31mthe\u001b[0m \u001b[1;31mtreatment\u001b[0m \u001b[1;31mof\u001b[0m large bilateral lesions \u001b[1;31mis\u001b[0m emphasized and discussed. \n",
      "    Publication: J Vasc Surg 8707; 5(4):648-50\n",
      "[19] docID: \u001b[34m87059178\t\u001b[0msimilarity/distance: \u001b[32m0.22819\u001b[0m\n",
      "    Title: \u001b[4mFlow cytometric sorting of human epidermal pure basal cell suspensions using a specific antikeratin monoclonal antibody.\u001b[0m\n",
      "\n",
      "    Authors: Staquet MJ; Albert J; Lawrence JJ; Thivolet J\n",
      "    Keywords: Antibodies, Monoclonal; Antibody Specificity; Cell Separation; Cytological Techniques; Epidermis; Female; Flow Cytometry; Human; Keratin; Support, Non-U.S. Gov't\n",
      "    Abstract: A method \u001b[1;31mis\u001b[0m described \u001b[1;31mfor\u001b[0m flow cytometric sorting \u001b[1;31mof\u001b[0m epidermal basal cells, based on \u001b[1;31mthe\u001b[0m cellular keratin content. KL1, a monoclonal antikeratin antibody specific \u001b[1;31mfor\u001b[0m \u001b[1;31mthe\u001b[0m 56.5 kD polypeptide present \u001b[1;31min\u001b[0m suprabasal cells was \u001b[1;31mused\u001b[0m to distinguish suprabasal from basal cells. After \u001b[1;31mthe\u001b[0m \u001b[1;31maction\u001b[0m \u001b[1;31mof\u001b[0m Triton X-100, epidermal cells were stained \u001b[1;31min\u001b[0m suspensions with KL1 antibody, and KL1-positive cells were separated by flow cytometry from KL1-negative cells (basal cells). This makes it \u001b[1;31mpossible\u001b[0m to sort and analyze pure fractions \u001b[1;31mof\u001b[0m human epidermal basal cells since no less than 99% \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m KL1 sorted cells were bearing bullous pemphigoid antigen. Therefore, \u001b[1;31mthe\u001b[0m use \u001b[1;31mof\u001b[0m specific antibodies to cytoplasmic antigens can be \u001b[1;31mof\u001b[0m help \u001b[1;31min\u001b[0m sorting pure fractions \u001b[1;31mof\u001b[0m cells \u001b[1;31mof\u001b[0m a particular tissue \u001b[1;31mfor\u001b[0m further studies. \n",
      "    Publication: J Invest Dermatol 8703; 87(6):792-4\n",
      "[20] docID: \u001b[34m87207411\t\u001b[0msimilarity/distance: \u001b[32m0.22836\u001b[0m\n",
      "    Title: \u001b[4mTreatment of \"difficult genitourinary tract infections\".\u001b[0m\n",
      "\n",
      "    Authors: Krieger JN\n",
      "    Keywords: Antibiotics; Antiprotozoal Agents; Chlamydia Infections; Female; Genital Diseases, Female; Genital Diseases, Male; Human; Male; Protozoan Infections; Trichomonas Vaginitis; Urinary Tract Infections\n",
      "    Abstract: \u001b[1;31mTreatment\u001b[0m \u001b[1;31mof\u001b[0m patients with \"urinary tract infections\" or related conditions \u001b[1;31mis\u001b[0m one \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m most common reasons \u001b[1;31mfor\u001b[0m urologic consultation. Some patients \u001b[1;31mhave\u001b[0m well-defined syndromes, \u001b[1;31msuch\u001b[0m as bacterial prostatitis or recurrent cystitis, with familiar algorithms \u001b[1;31mfor\u001b[0m evaluation and \u001b[1;31mtreatment.\u001b[0m Unfortunately, many patients \u001b[1;31mhave\u001b[0m \"difficult genitourinary tract infections\" \u001b[1;31mthat\u001b[0m challenge \u001b[1;31mthe\u001b[0m diagnostic acumen \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m urologist and, all too often, lead to frustration \u001b[1;31mof\u001b[0m \u001b[1;31mthe\u001b[0m patient. \u001b[1;31mAlthough\u001b[0m making an etiologic diagnosis may be \u001b[1;31mimpossible\u001b[0m \u001b[1;31min\u001b[0m every case, an organized approach to evaluation and \u001b[1;31mtreatment\u001b[0m may avoid unnecessary or inappropriate therapy and patient dissatisfaction. \n",
      "    Publication: Urol Clin North Am 8708; 14(2):257-64\n"
     ]
    }
   ],
   "source": [
    "query_document(query, field='content', model_type='wmd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
